# ## Objectives
# 
# In this 4 part assignment i will:
# 
# - Part 1 ETL
#   - Load a csv dataset
#   - Remove duplicates if any
#   - Drop rows with null values if any
#   - Make transformations
#   - Store the cleaned data in parquet format
# - Part 2 Machine Learning Pipeline creation
#   - Create a machine learning pipeline for prediction
# - Part 3 Model evaluation
#   - Evaluate the model using metrics
#   - Print the intercept and the coefficients
# - Part 4 Model Persistance
#   - Cave the model for future production use
#   - Load and verify the stored model



### Datasets
#  - Modified version of car mileage dataset. Original dataset available at https://archive.ics.uci.edu/ml/datasets/auto+mpg 



#!pip install pyspark==3.1.2 -q
#!pip install findspark -q



### Importing Required Libraries
# i can also use this section to suppress warnings generated by code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# FindSpark simplifies the process of using Apache Spark with Python
import findspark
findspark.init()


## Part 1 - ETL


### Task 1 - Import required libraries

#import functions/Classes for sparkml
# import Classes for pipeline creation
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.pipeline import PipelineModel
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import StandardScaler
from pyspark.ml.evaluation import RegressionEvaluator
import wget
import os



### Task 2 - Create a spark session
spark = SparkSession.builder.appName("Spark project 1").getOrCreate()



### Task 3 - Load the csv file into a dataframe
# Download the data file
wget.download('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/mpg-raw.csv')
df = spark.read.csv("mpg-raw.csv", header=True, inferSchema=True)   # Loading dataset


### Task 4 - Print top 5 rows of the dataset
df.show(5)

### Task 5 - Print the number of cars in each Origin
df.groupBy('Origin').count().show()


### Task 6 - Print the total number of rows in the dataset
rowcount1 = df.count()
print(rowcount1)


### Task 7 - Drop all the duplicate rows from the dataset
df.dropDuplicates()


### Task 8 - Print the total number of rows in the dataset
rowcount2 = df.count()
print(rowcount2)  # comparing count of rows before and after dropping duplicates shows that there were no duplicates


### Task 9 - Drop all the rows that contain null values from the dataset
df = df.na.drop()


### Task 10 - Print the total number of rows in the dataset
rowcount3 = df.count()
print(rowcount3)


### Task 11 - Rename the column "Engine Disp" to "Engine_Disp"Drop
df = df.withColumnRenamed('Engine Disp','Engine_Disp')


### Task 12 - Save the dataframe in parquet format, name the file as "mpg-cleaned.parquet"
df.write.parquet("mpg-cleaned.parquet")


#### Part 1 - Evaluation
print("Part 1 - Evaluation")
print("Total rows = ", rowcount1)
print("Total rows after dropping duplicate rows = ", rowcount2)
print("Total rows after dropping duplicate rows and rows with null values = ", rowcount3)
print("Renamed column name = ", df.columns[2])
print("mpg-cleaned.parquet exists :", os.path.isdir("mpg-cleaned.parquet"))




### Part - 2 Machine Learning Pipeline creation

### Task 1 - Load data from "mpg-cleaned.parquet" into a dataframe
df = spark.read.parquet("mpg-cleaned.parquet")
rowcount4 = df.count()


#show top 5 rows
df.show(5)


#print the schema of the dataframe
df.printSchema()


### Task 2 - Define the StringIndexer pipeline stage
# Stage - 1 Using StringIndexer convert the string column "Origin" into "OriginIndex"
indexer = StringIndexer(inputCol="Origin", outputCol="OriginIndex")


### Task 3 - Define the VectorAssembler pipeline stage
# Stage 2 - assemble the input columns 'Cylinders','Engine_Disp','Horsepower','Weight','Accelerate','Year' into a single column "features"
assembler = VectorAssembler(inputCols=["Cylinders", "Engine_Disp", "Horsepower", "Weight", "Accelerate", "Year"], outputCol="features")



### Task 4 - Define the StandardScaler pipeline stage
# Stage 3 - scale the "features" using standard scaler and store in "scaledFeatures" column
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")


### Task 5 - Define the LinearRegression pipeline stage
# Stage 4 - Create a LinearRegression stage to predict "MPG"
lr = LinearRegression(featuresCol="scaledFeatures", labelCol="MPG")


### Task 6 - Build the pipeline
pipeline = Pipeline(stages=[indexer, assembler, scaler, lr])   # Building a pipeline using the above four stages


### Task 7 - Split the data 
# Split the data into training and testing sets with 70:30 split. Use 42 as seed
(trainingData, testingData) = df.randomSplit([0.7, 0.3], seed=42)


### Task 8 - Fit the pipeline
# Fit the pipeline using the training data
pipelineModel = pipeline.fit(trainingData)


#### Part 2 - Evaluation
print("Part 2 - Evaluation")
print("Total rows = ", rowcount4)
ps = [str(x).split("_")[0] for x in pipeline.getStages()] # creating list of all pipeline stages using list comprehension
print("Pipeline Stage 1 = ", ps[0])
print("Pipeline Stage 2 = ", ps[1])
print("Pipeline Stage 3 = ", ps[2])
print("Label column = ", lr.getLabelCol())


# Make predictions on testing data
predictions = pipelineModel.transform(testingData)


### Task 2 - Print the MSE
evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="MPG", metricName="mse")
mse = evaluator.evaluate(predictions)
print(mse)


### Task 3 - Print the MAE
evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="MPG", metricName="mae")
mae = evaluator.evaluate(predictions)
print(mae)


### Task 4 - Print the R-Squared(R2)
evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="MPG", metricName="r2")
r2 = evaluator.evaluate(predictions)
print(r2)


#### Part 3 - Evaluation
print("Part 3 - Evaluation")
print("Mean Squared Error = ", round(mse,2))
print("Mean Absolute Error = ", round(mae,2))
print("R Squared = ", round(r2,2))
lrModel = pipelineModel.stages[-1]
print("Intercept = ", round(lrModel.intercept,2))



### Part 4 - Model persistance

### Task 1 - Save the model to the path "Spark_Project_1"
# Save the pipeline model
pipelineModel.write().save("Spark_Project_1")


### Task 2 - Load the model from the path "Spark_Project_1"
# Load the pipeline model
loadedPipelineModel = PipelineModel.load("Spark_Project_1")


### Task 3 - Make predictions using the loaded model on the test data
# Use the loaded pipeline model for predictions
predictions = loadedPipelineModel.transform(testingData)


### Task 4 - Show the predictions
predictions.select("MPG","prediction").show()


#### Part 4 - Evaluation
print("Part 4 - Evaluation")

loadedmodel = loadedPipelineModel.stages[-1]
totalstages = len(loadedPipelineModel.stages)
inputcolumns = loadedPipelineModel.stages[1].getInputCols()

print("Number of stages in the pipeline = ", totalstages)
for i,j in zip(inputcolumns, loadedmodel.coefficients):
    print(f"Coefficient for {i} is {round(j,4)}")

### Task 5 - Stop Spark Session
spark.stop()

